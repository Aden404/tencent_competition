{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重新提取了原数据集的0.25   baseline :  0.715596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、单独加入广告发布的不同用户数, 0.717474，效果有较大提升\n",
    "\n",
    "2、加入兴趣特征， 0.716473，效果下降\n",
    "\n",
    "3、加入年龄分布，0.717952，效果上升\n",
    "\n",
    "4、加入教育分布,  0.715367，效果下降\n",
    "\n",
    "5、加入消费能力分布，\n",
    "\n",
    "4、删掉兴趣特征，0.717499，下降\n",
    "\n",
    "5、加入广告点击率， 0.717974，效果上升\n",
    "\n",
    "6、加入LBS历史点击率， 0.717536,效果下降\n",
    "\n",
    "7、删掉广告点击率， 0.717347, 效果下降\n",
    "\n",
    "\n",
    "6、分析广告投放和地理位置的关系\n",
    "\n",
    "7、加入每个地理位置的人数特征, 效果下降\n",
    "\n",
    "15、分析每个广告对应的地理位置分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./datasets/train_data2.csv')\n",
    "target = pd.read_csv('./datasets/train_target2.csv', names=['label'], header=None)\n",
    "\n",
    "data.drop(data.columns[[0]], axis=1, inplace=True)  #删除某列函数\n",
    "target = target.reset_index(drop=True)\n",
    "\n",
    "one_hot_feature=['creativeSize', 'LBS','age','carrier','consumptionAbility', 'education','gender','house','os','marriageStatus','advertiserId','campaignId', 'creativeId',\n",
    "       'adCategoryId', 'productId', 'productType']\n",
    "vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']\n",
    "\n",
    "\n",
    "\n",
    "for feature in one_hot_feature:\n",
    "    try:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))\n",
    "    except:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split = StratifiedShuffleSplit(n_splits=1, test_size=0.12, random_state=42)\n",
    "y = np.array(target).squeeze()\n",
    "# for train_index, test_index in split.split(data,  y):\n",
    "#     X_train = data.iloc[train_index]\n",
    "#     X_test = data.iloc[test_index]\n",
    "#     y_train = y[train_index]\n",
    "#     y_test = y[test_index]\n",
    "X_train,X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clicked = X_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理联网类型特征\n",
    "ct_train = X_train['ct'].values\n",
    "ct_train = [m.split(' ') for m in ct_train]\n",
    "ct_trains = []\n",
    "for i in ct_train:\n",
    "    index = [0, 0, 0, 0, 0]\n",
    "    for j in i:\n",
    "        index[int(j)] = 1\n",
    "    ct_trains.append(index)\n",
    "\n",
    "ct_test = X_test['ct'].values\n",
    "ct_test = [m.split(' ') for m in ct_test]\n",
    "ct_tests = []\n",
    "for i in ct_test:\n",
    "    index = [0, 0, 0, 0, 0]\n",
    "    for j in i:\n",
    "        index[int(j)] = 1\n",
    "    ct_tests.append(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加每个广告点击率特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ad = X_train['aid'].value_counts().sort_index()\n",
    "num_ad_clicked = data_clicked['aid'].value_counts().sort_index()\n",
    "\n",
    "ratio = num_ad_clicked / num_ad\n",
    "\n",
    "ratio_clicked = pd.DataFrame({\n",
    "    'aid': ratio.index,\n",
    "    'ratio_clicked' : ratio.values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_train, ratio_clicked, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, ratio_clicked, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加每个广告出现次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ad_post = X_train['aid'].value_counts().sort_index()\n",
    "\n",
    "num_ad_post = pd.DataFrame({\n",
    "    'aid': num_ad_post.index,\n",
    "    'num_ad_post' : num_ad_post.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_train, num_ad_post, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, num_ad_post, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加每个广告推送给不同的用户数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_advertise_touser = X_train.groupby('aid').uid.nunique()\n",
    "num_advertise_touser = pd.DataFrame({\n",
    "    'aid': num_advertise_touser.index,\n",
    "    'num_advertise_touser' : num_advertise_touser.values\n",
    "})\n",
    "X_train = pd.merge(X_train, num_advertise_touser, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, num_advertise_touser, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入推广计划转化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_campaign = X_train['campaignId'].value_counts().sort_index()\n",
    "num_campaign_clicked = data_clicked['campaignId'].value_counts().sort_index()\n",
    "\n",
    "ratio_num_campaign = num_campaign_clicked / num_campaign\n",
    "\n",
    "ratio_num_campaign = pd.DataFrame({\n",
    "    'campaignId': ratio_num_campaign.index,\n",
    "    'ratio_num_campaign' : ratio_num_campaign.values\n",
    "})\n",
    "X_train = pd.merge(X_train, ratio_num_campaign, on=['campaignId'], how='left')\n",
    "X_test = pd.merge(X_test, ratio_num_campaign, on=['campaignId'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入用户兴趣总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_interest = X_train[['interest1', 'interest2', 'interest5', 'kw1', 'kw2', 'topic1', 'topic2']]\n",
    "test_all_interest = X_test[['interest1', 'interest2', 'interest5', 'kw1', 'kw2', 'topic1', 'topic2']]\n",
    "num_all_interest_train = []\n",
    "num_all_interest_test = []\n",
    "aids = []\n",
    "train_array = np.array(train_all_interest)\n",
    "test_array = np.array(test_all_interest)\n",
    "\n",
    "for i in range(train_array.shape[0]):\n",
    "    num = 0\n",
    "    inter = train_array[i]\n",
    "    for j in inter:\n",
    "        inter_lis = j.split(' ')\n",
    "        if inter_lis[0] == '-1':\n",
    "            continue\n",
    "        num += len(inter_lis)\n",
    "    num_all_interest_train.append(num)\n",
    "    \n",
    "    \n",
    "for i in range(test_array.shape[0]):\n",
    "    num = 0\n",
    "    inter = test_array[i]\n",
    "    for j in inter:\n",
    "        inter_lis = j.split(' ')\n",
    "        if inter_lis[0] == '-1':\n",
    "            continue\n",
    "        num += len(inter_lis)\n",
    "    num_all_interest_test.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_all_interest_train = pd.DataFrame(num_all_interest_train, columns=['num_all_interest'])\n",
    "num_all_interest_test = pd.DataFrame(num_all_interest_test, columns=['num_all_interest'])\n",
    "\n",
    "X_train = pd.concat([X_train, num_all_interest_train], axis=1)\n",
    "X_test = pd.concat([X_test, num_all_interest_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析每个广告对应的人群年龄分布, 教育水平分布，消费能力分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_toother(typename):\n",
    "    num_ad_totype = X_train.groupby('aid')[typename].value_counts()\n",
    "    num_ad_totype_clicked = data_clicked.groupby('aid')[typename].value_counts()\n",
    "    ratio_num_ad_totype = num_ad_totype_clicked / num_ad_totype\n",
    "    list_num_ad_totype = []\n",
    "    num_adid = X_train['aid'].value_counts().sort_index().index\n",
    "    for aid_out in num_adid:\n",
    "        dict_buf = {}\n",
    "        dict_num_ad_totype = {}\n",
    "        dict_num_ad_totype['aid'] = aid_out\n",
    "        for i, j in ratio_num_ad_totype.items():\n",
    "            aid = i[0]\n",
    "            feature = i[1]\n",
    "            if(aid == aid_out):\n",
    "                dict_buf[feature] = float(\"%.5f\" % j)\n",
    "        fea_name = 'num_ad_to'+typename\n",
    "        dict_num_ad_totype[fea_name] = dict_buf\n",
    "        list_num_ad_totype.append(dict_num_ad_totype)\n",
    "    return list_num_ad_totype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_ad_toedu = get_ad_toother('education')\n",
    "list_num_ad_toedu = pd.DataFrame(list_num_ad_toedu)\n",
    "X_train = pd.merge(X_train, list_num_ad_toedu, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_toedu, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_toedu'] = [ j.get(i, 0) for i, j in X_train[['education', 'num_ad_toeducation']].values]\n",
    "X_test['ratio_num_ad_toedu'] = [ j.get(i, 0) for i, j in X_test[['education', 'num_ad_toeducation']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_num_ad_toage = get_ad_toother('age')\n",
    "list_num_ad_toage = pd.DataFrame(list_num_ad_toage)\n",
    "X_train = pd.merge(X_train, list_num_ad_toage, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_toage, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_toage'] = [ j.get(i, 0) for i, j in X_train[['age', 'num_ad_toage']].values]\n",
    "X_test['ratio_num_ad_toage'] = [ j.get(i, 0) for i, j in X_test[['age', 'num_ad_toage']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_ad_toconsume = get_ad_toother('consumptionAbility')\n",
    "list_num_ad_toconsume = pd.DataFrame(list_num_ad_toconsume)\n",
    "X_train = pd.merge(X_train, list_num_ad_toconsume, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_toconsume, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_toconsume'] = [ j.get(i, 0) for i, j in X_train[['consumptionAbility', 'num_ad_toconsumptionAbility']].values]\n",
    "X_test['ratio_num_ad_toconsume'] = [ j.get(i, 0) for i, j in X_test[['consumptionAbility', 'num_ad_toconsumptionAbility']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入地理位置所对应的用户人数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lbs_user = X_train.groupby('LBS').uid.nunique()\n",
    "num_lbs_user = pd.DataFrame({\n",
    "    'LBS': num_lbs_user.index,\n",
    "    'num_lbs_user' : num_lbs_user.values\n",
    "})\n",
    "X_train = pd.merge(X_train, num_lbs_user, on=['LBS'], how='left')\n",
    "X_test = pd.merge(X_test, num_lbs_user, on=['LBS'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入用户所在LBS的历史点击率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lbs = X_train.groupby('LBS').uid.nunique()\n",
    "num_lbs_clicked = data_clicked.groupby('LBS').uid.nunique()\n",
    "ratio_num_lbs = num_lbs_clicked / num_lbs\n",
    "ratio_num_lbs = ratio_num_lbs.fillna(0)\n",
    "ratio_num_lbs = pd.DataFrame({\n",
    "    'LBS': ratio_num_lbs.index,\n",
    "    'ration_num_LBS' : ratio_num_lbs.values\n",
    "})\n",
    "X_train = pd.merge(X_train, ratio_num_lbs, on=['LBS'], how='left')\n",
    "X_test = pd.merge(X_test, ratio_num_lbs, on=['LBS'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入用户的历史接收的广告次数与历史点击次数，以及历史点击率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_getad = X_train.groupby('uid').aid.nunique().sort_index()\n",
    "num_user_clicked = data_clicked.groupby('uid').aid.nunique().sort_index()\n",
    "\n",
    "num_user_getad = pd.DataFrame({\n",
    "    'uid': num_user_getad.index,\n",
    "    'num_user_getad' : num_user_getad.values\n",
    "})\n",
    "\n",
    "num_user_clicked = pd.DataFrame({\n",
    "    'uid': num_user_clicked.index,\n",
    "    'num_user_clicked' : num_user_clicked.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_train, num_user_clicked, on=['uid'], how='left')\n",
    "X_test = pd.merge(X_test, num_user_clicked, on=['uid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_train_encoded= np.hstack((X_train_encoded, ct_trains))\n",
    "# X_test_encoded= np.hstack((X_test_encoded, ct_tests))\n",
    "\n",
    "# , 'num_common_interest2', 'num_common_interest1', 'num_common_interest5' 'num_advertise_touser' 'ration_num_LBS' \n",
    "X_train_encoded = ct_trains\n",
    "X_test_encoded = ct_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "oc_encoder = OneHotEncoder()\n",
    "for feature in one_hot_feature:\n",
    "    oc_encoder.fit(data[feature].reshape(-1, 1))\n",
    "    train_a=oc_encoder.transform(X_train[feature].values.reshape(-1, 1))\n",
    "    X_train_encoded = sparse.hstack((X_train_encoded, train_a))\n",
    "    test_a=oc_encoder.transform(X_test[feature].values.reshape(-1, 1))\n",
    "    X_test_encoded = sparse.hstack((X_test_encoded, test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(703905, 1183)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']\n",
    "ct_encoder = CountVectorizer(min_df=0.0009)\n",
    "for feature in vector_feature:\n",
    "    ct_encoder.fit(data[feature])\n",
    "    train_a = ct_encoder.transform(X_train[feature])\n",
    "    X_train_encoded = sparse.hstack((X_train_encoded, train_a))\n",
    "    test_a = ct_encoder.transform(X_test[feature])\n",
    "    X_test_encoded = sparse.hstack((X_test_encoded, test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-94a151758c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m scaler.fit(X_train[['num_advertise_touser','num_common_interest2', 'num_common_interest1', 'num_common_interest5', 'num_common_topic1',\n\u001b[1;32m----> 5\u001b[1;33m                       'ratio_num_ad_toage', 'ratio_num_ad_toconsume']].values)\n\u001b[0m\u001b[0;32m      6\u001b[0m train_encoded = scaler.transform(X_train[['num_advertise_touser','num_common_interest2', 'num_common_interest1', 'num_common_interest5', \n\u001b[0;32m      7\u001b[0m                       'num_common_topic1', 'ratio_num_ad_toage', 'ratio_num_ad_toconsume']].values)\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[['num_advertise_touser','num_common_interest2', 'num_common_interest1', 'num_common_interest5', 'num_common_topic1',\n",
    "                      'ratio_num_ad_toage', 'ratio_num_ad_toconsume']].values)\n",
    "train_encoded = scaler.transform(X_train[['num_advertise_touser','num_common_interest2', 'num_common_interest1', 'num_common_interest5', \n",
    "                      'num_common_topic1', 'ratio_num_ad_toage', 'ratio_num_ad_toconsume']].values)\n",
    "\n",
    "test_encoded = scaler.transform(X_test[['num_advertise_touser','num_common_interest2', 'num_common_interest1', 'num_common_interest5', \n",
    "                        'num_common_topic1', 'ratio_num_ad_toage', 'ratio_num_ad_toconsume']].values)\n",
    "\n",
    "# scaler.fit(X_train[['num_advertise_touser']].values)\n",
    "# train_encoded = scaler.transform(X_train[['num_advertise_touser']].values)\n",
    "# test_encoded = scaler.transform(X_test[['num_advertise_touser']].values)  'ratio_num_ad_tocedu'\n",
    "# ,'num_common_interest2', 'num_common_interest1', 'num_common_interest5', 'num_common_topic1'  'ratio_num_ad_toconsume' \n",
    "X_train_encoded2= sparse.hstack((X_train_encoded, train_encoded))\n",
    "X_test_encoded2= sparse.hstack((X_test_encoded, test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-3d0bf67b5a65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     )\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_encoded2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_encoded2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    677\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    471\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1519\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1521\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# y_train = np.array(target).squeeze()\n",
    "clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=0.9,\n",
    "        max_depth=-1, n_estimators=300, objective='binary',\n",
    "        subsample=0.7, colsample_bytree=0.8, subsample_freq=1,\n",
    "        learning_rate=0.05, min_child_weight=60, random_state=2018, n_jobs=-1\n",
    "    )\n",
    "clf.fit(X_train_encoded2, y_train, eval_set=[(X_test_encoded2, y_test)], eval_metric='auc',early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_interest(type_name, ratio):\n",
    "    num_adid = data_clicked['aid'].value_counts().sort_index().index\n",
    "    num_aid_clicked = dict(data_clicked['aid'].value_counts().sort_index())\n",
    "    num_user_clicksameAd_interest = data_clicked.groupby('aid')[type_name].value_counts()\n",
    "    dict_interest = {}\n",
    "    for adid in num_adid:\n",
    "        dict_buf = {}\n",
    "        for interest in num_user_clicksameAd_interest.items():\n",
    "            index = interest[0]\n",
    "            if index[0] == adid:\n",
    "                number = interest[1]\n",
    "                detail = index[1]\n",
    "                detail = detail.split(' ')\n",
    "                for det in detail:\n",
    "                    if det not in dict_buf:\n",
    "                        dict_buf[det] = number\n",
    "                    else:\n",
    "                        dict_buf[det] += number\n",
    "        dict_interest[adid] = dict_buf\n",
    "    dict_common_interest = []\n",
    "    for adid, dict_inter in dict_interest.items():\n",
    "        dict_common_buf = {}\n",
    "        dict_common_buf['aid'] = adid\n",
    "        common_inter = []\n",
    "        ad_total = num_aid_clicked[adid] - dict_inter.get('-1', 0)\n",
    "        if '-1' in dict_inter:\n",
    "            dict_inter.pop('-1')\n",
    "        for id_inter, num in dict_inter.items():\n",
    "            if num >= ad_total*ratio:\n",
    "                common_inter.append(id_inter)\n",
    "        str_name = 'common_'+type_name\n",
    "        dict_common_buf[str_name] = common_inter\n",
    "        dict_common_interest.append(dict_common_buf)\n",
    "    return dict_common_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取相同的兴趣ID2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_interest2 = get_common_interest('interest2', 0.25)\n",
    "df_common_interest2 = pd.DataFrame(dict_common_interest2)\n",
    "X_train = pd.merge(X_train, df_common_interest2, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_interest2, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取相同的兴趣ID1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_interest1 = get_common_interest('interest1', 0.25)\n",
    "df_common_interest1 = pd.DataFrame(dict_common_interest1)\n",
    "X_train = pd.merge(X_train, df_common_interest1, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_interest1, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取相同的兴趣ID5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_interest5 = get_common_interest('interest5', 0.25)\n",
    "df_common_interest5 = pd.DataFrame(dict_common_interest5)\n",
    "X_train = pd.merge(X_train, df_common_interest5, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_interest5, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_interest2'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_train[['interest2', 'common_interest2']].values]\n",
    "X_test['num_common_interest2'] = [len(set(i.split(' ')).intersection(set(j))) /len(j) for i, j in X_test[['interest2', 'common_interest2']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_interest1'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_train[['interest1', 'common_interest1']].values]\n",
    "X_test['num_common_interest1'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_test[['interest1', 'common_interest1']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_interest5'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_train[['interest5', 'common_interest5']].values]\n",
    "X_test['num_common_interest5'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_test[['interest5', 'common_interest5']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_all'] = X_train[['num_common_interest5', 'num_common_interest2', 'num_common_interest1']].sum(axis=1)\n",
    "X_test['num_common_all'] = X_test[['num_common_interest5', 'num_common_interest2', 'num_common_interest1']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取关键字1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_kw1 = get_common_interest('kw1', 0.05)\n",
    "df_common_kw1 = pd.DataFrame(dict_common_kw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_train, df_common_kw1, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_kw1, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_kw1'] = [len(set(i.split(' ')).intersection(set(j))) / (len(j)+1) for i, j in X_train[['kw1', 'common_kw1']].values ]\n",
    "X_test['num_common_kw1'] = [len(set(i.split(' ')).intersection(set(j))) /  (len(j)+1) for i, j in X_test[['kw1', 'common_kw1']].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_kw2 = get_common_interest('kw2', 0.05)\n",
    "df_common_kw2 = pd.DataFrame(dict_common_kw2)\n",
    "X_train = pd.merge(X_train, df_common_kw2, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_kw2, on=['aid'], how='left')\n",
    "X_train['num_common_kw2'] = [len(set(i.split(' ')).intersection(set(j))) / (len(j)+1) for i, j in X_train[['kw2', 'common_kw2']].values ]\n",
    "X_test['num_common_kw2'] = [len(set(i.split(' ')).intersection(set(j))) /  (len(j)+1) for i, j in X_test[['kw2', 'common_kw2']].values ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取主题1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_topic1 = get_common_interest('topic1', 0.1)\n",
    "df_common_topic1 = pd.DataFrame(dict_common_topic1)\n",
    "X_train = pd.merge(X_train, df_common_topic1, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_topic1, on=['aid'], how='left')\n",
    "X_train['num_common_topic1'] = [len(set(i.split(' ')).intersection(set(j))) / (len(j)+1) for i, j in X_train[['topic1', 'common_topic1']].values ]\n",
    "X_test['num_common_topic1'] = [len(set(i.split(' ')).intersection(set(j))) /  (len(j)+1) for i, j in X_test[['topic1', 'common_topic1']].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_topic2 = get_common_interest('topic2', 0.05)\n",
    "df_common_topic2 = pd.DataFrame(dict_common_topic2)\n",
    "X_train = pd.merge(X_train, df_common_topic2, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_topic2, on=['aid'], how='left')\n",
    "X_train['num_common_topic2'] = [len(set(i.split(' ')).intersection(set(j))) / (len(j)+1) for i, j in X_train[['topic2', 'common_topic2']].values ]\n",
    "X_test['num_common_topic2'] = [len(set(i.split(' ')).intersection(set(j))) /  (len(j)+1) for i, j in X_test[['topic2', 'common_topic2']].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid</th>\n",
       "      <th>uid</th>\n",
       "      <th>advertiserId</th>\n",
       "      <th>campaignId</th>\n",
       "      <th>creativeId</th>\n",
       "      <th>creativeSize</th>\n",
       "      <th>adCategoryId</th>\n",
       "      <th>productId</th>\n",
       "      <th>productType</th>\n",
       "      <th>LBS</th>\n",
       "      <th>...</th>\n",
       "      <th>num_ad_toeducation</th>\n",
       "      <th>ratio_num_ad_toedu</th>\n",
       "      <th>num_ad_toage</th>\n",
       "      <th>ratio_num_ad_toage</th>\n",
       "      <th>common_interest2</th>\n",
       "      <th>common_interest1</th>\n",
       "      <th>common_interest5</th>\n",
       "      <th>num_common_interest2</th>\n",
       "      <th>num_common_interest1</th>\n",
       "      <th>num_common_interest5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>411</td>\n",
       "      <td>25980816</td>\n",
       "      <td>36</td>\n",
       "      <td>83</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>573</td>\n",
       "      <td>...</td>\n",
       "      <td>{0: 0.09407, 1: 0.05341, 2: 0.05353, 3: 0.0502...</td>\n",
       "      <td>0.05029</td>\n",
       "      <td>{0: 0.18, 1: 0.03039, 2: 0.06225, 3: 0.04074, ...</td>\n",
       "      <td>0.06225</td>\n",
       "      <td>[30, 73, 70, 46]</td>\n",
       "      <td>[36, 11, 70, 76, 109, 75, 29, 46, 49]</td>\n",
       "      <td>[46, 6, 59, 131, 8, 76, 93, 116, 132, 121, 129...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1918</td>\n",
       "      <td>7431028</td>\n",
       "      <td>78</td>\n",
       "      <td>128</td>\n",
       "      <td>162</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>676</td>\n",
       "      <td>...</td>\n",
       "      <td>{0: 0.10204, 1: 0.05081, 2: 0.08037, 3: 0.0639...</td>\n",
       "      <td>0.04388</td>\n",
       "      <td>{1: 0.09115, 2: 0.17875, 3: 0.04918, 4: 0.0378...</td>\n",
       "      <td>0.04945</td>\n",
       "      <td>[24, 30, 52, 21, 73, 74, 70]</td>\n",
       "      <td>[36, 11, 75, 29, 70, 76, 93, 46, 109, 28, 77, ...</td>\n",
       "      <td>[100, 46, 37, 6, 59, 80, 129, 78, 30, 131, 8, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1468</td>\n",
       "      <td>4312799</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>152</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>367</td>\n",
       "      <td>...</td>\n",
       "      <td>{0: 0.03614, 1: 0.05738, 2: 0.07126, 3: 0.0717...</td>\n",
       "      <td>0.07126</td>\n",
       "      <td>{0: 0.11111, 1: 0.05028, 2: 0.10325, 3: 0.0239...</td>\n",
       "      <td>0.01995</td>\n",
       "      <td>[24, 30, 73, 9, 70, 10, 46, 4]</td>\n",
       "      <td>[36, 11, 70, 76, 75, 29, 109, 119, 47, 9, 93, ...</td>\n",
       "      <td>[100, 46, 37, 131, 8, 6, 59, 78, 30, 109, 50, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>960</td>\n",
       "      <td>69244796</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>{0: 0.02913, 1: 0.04698, 2: 0.05079, 3: 0.0595...</td>\n",
       "      <td>0.05841</td>\n",
       "      <td>{0: nan, 1: 0.05945, 2: 0.05981, 3: 0.05433, 4...</td>\n",
       "      <td>0.04044</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[36, 11, 70, 76, 75, 29, 93, 46, 109, 59, 49, ...</td>\n",
       "      <td>[100, 131, 37, 116, 8, 50, 6, 42, 46, 121, 59,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>660</td>\n",
       "      <td>1747755</td>\n",
       "      <td>76</td>\n",
       "      <td>123</td>\n",
       "      <td>106</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>594</td>\n",
       "      <td>...</td>\n",
       "      <td>{0: 0.09091, 1: 0.06395, 2: 0.04815, 3: 0.0336...</td>\n",
       "      <td>0.05698</td>\n",
       "      <td>{1: 0.06029, 2: 0.07923, 3: 0.03609, 4: 0.0431...</td>\n",
       "      <td>0.05652</td>\n",
       "      <td>[73, 24, 30, 9, 21, 6, 46, 70, 4]</td>\n",
       "      <td>[75, 29, 36, 11, 109, 49, 6, 114, 116, 59, 118...</td>\n",
       "      <td>[100, 109, 6, 46, 59, 116, 13, 71, 132, 121, 1...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aid       uid  advertiserId  campaignId  creativeId  creativeSize  \\\n",
       "0   411  25980816            36          83          21             8   \n",
       "1  1918   7431028            78         128         162             6   \n",
       "2  1468   4312799            12           3         152             6   \n",
       "3   960  69244796             9          48         159             5   \n",
       "4   660   1747755            76         123         106            12   \n",
       "\n",
       "   adCategoryId  productId  productType  LBS         ...           \\\n",
       "0             6          0            0  573         ...            \n",
       "1             1          0            0  676         ...            \n",
       "2            16          0            0  367         ...            \n",
       "3             4         14            3  156         ...            \n",
       "4             6          0            0  594         ...            \n",
       "\n",
       "                                  num_ad_toeducation ratio_num_ad_toedu  \\\n",
       "0  {0: 0.09407, 1: 0.05341, 2: 0.05353, 3: 0.0502...            0.05029   \n",
       "1  {0: 0.10204, 1: 0.05081, 2: 0.08037, 3: 0.0639...            0.04388   \n",
       "2  {0: 0.03614, 1: 0.05738, 2: 0.07126, 3: 0.0717...            0.07126   \n",
       "3  {0: 0.02913, 1: 0.04698, 2: 0.05079, 3: 0.0595...            0.05841   \n",
       "4  {0: 0.09091, 1: 0.06395, 2: 0.04815, 3: 0.0336...            0.05698   \n",
       "\n",
       "                                        num_ad_toage  ratio_num_ad_toage  \\\n",
       "0  {0: 0.18, 1: 0.03039, 2: 0.06225, 3: 0.04074, ...             0.06225   \n",
       "1  {1: 0.09115, 2: 0.17875, 3: 0.04918, 4: 0.0378...             0.04945   \n",
       "2  {0: 0.11111, 1: 0.05028, 2: 0.10325, 3: 0.0239...             0.01995   \n",
       "3  {0: nan, 1: 0.05945, 2: 0.05981, 3: 0.05433, 4...             0.04044   \n",
       "4  {1: 0.06029, 2: 0.07923, 3: 0.03609, 4: 0.0431...             0.05652   \n",
       "\n",
       "                    common_interest2  \\\n",
       "0                   [30, 73, 70, 46]   \n",
       "1       [24, 30, 52, 21, 73, 74, 70]   \n",
       "2     [24, 30, 73, 9, 70, 10, 46, 4]   \n",
       "3                               [30]   \n",
       "4  [73, 24, 30, 9, 21, 6, 46, 70, 4]   \n",
       "\n",
       "                                    common_interest1  \\\n",
       "0              [36, 11, 70, 76, 109, 75, 29, 46, 49]   \n",
       "1  [36, 11, 75, 29, 70, 76, 93, 46, 109, 28, 77, ...   \n",
       "2  [36, 11, 70, 76, 75, 29, 109, 119, 47, 9, 93, ...   \n",
       "3  [36, 11, 70, 76, 75, 29, 93, 46, 109, 59, 49, ...   \n",
       "4  [75, 29, 36, 11, 109, 49, 6, 114, 116, 59, 118...   \n",
       "\n",
       "                                    common_interest5  num_common_interest2  \\\n",
       "0  [46, 6, 59, 131, 8, 76, 93, 116, 132, 121, 129...                   0.0   \n",
       "1  [100, 46, 37, 6, 59, 80, 129, 78, 30, 131, 8, ...                   0.0   \n",
       "2  [100, 46, 37, 131, 8, 6, 59, 78, 30, 109, 50, ...                   0.0   \n",
       "3  [100, 131, 37, 116, 8, 50, 6, 42, 46, 121, 59,...                   0.0   \n",
       "4  [100, 109, 6, 46, 59, 116, 13, 71, 132, 121, 1...                   0.0   \n",
       "\n",
       "   num_common_interest1 num_common_interest5  \n",
       "0              0.333333             0.000000  \n",
       "1              0.550000             0.375000  \n",
       "2              0.210526             0.571429  \n",
       "3              0.823529             0.233333  \n",
       "4              0.714286             0.687500  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.drop(['common_topic1_x','common_topic1_y'] ,axis=1, inplace=True)\n",
    "# X_test.drop(['common_topic1_x','common_topic1_y'] ,axis=1, inplace=True)\n",
    "\n",
    "# X_test.drop(['common_interest2_x','common_interest1_x','interest2_new','interest1_new','common_interest2_y'\n",
    "#              ,'common_interest1_y','common_interest2','common_interest1'], axis=1, inplace=True)\n",
    "\n",
    "# X_test.drop(['num_common_interest','common_interest1_x','num_common_interest1','common_interest1_y','common_interest1','common_interest2_y'],\n",
    "#             axis=1, inplace=True)\n",
    "# X_train.drop(['num_common_interest','common_interest1_x','num_common_interest1','common_interest1_y','common_interest1','common_interest2_y'],\n",
    "#             axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 1 70 ['30', '73', '70']\n"
     ]
    }
   ],
   "source": [
    "for i, j in X_train[['interest2', 'common_interest2']].values:\n",
    "    print(i, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interest1_new = []\n",
    "for i in train_user_interest1.values:\n",
    "    inter2_buf = []\n",
    "    int2 = i[0].split(' ')\n",
    "    common = i[1]\n",
    "    for det in int2:\n",
    "        if det in common:\n",
    "            inter2_buf.append(det)\n",
    "    inter2_buf.extend(int2)\n",
    "    train_interest1_new.append(inter2_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interest1_new = []\n",
    "for i in test_user_interest1.values:\n",
    "    inter2_buf = []\n",
    "    int2 = i[0].split(' ')\n",
    "    common = i[1]\n",
    "    for det in int2:\n",
    "        if det in common:\n",
    "            inter2_buf.append(det)\n",
    "    inter2_buf.extend(int2)\n",
    "    test_interest1_new.append(inter2_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interest2_new = [' '.join(m) for m in train_interest2_new]\n",
    "X_train['interest2_new'] = train_interest2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interest2_new = [' '.join(m) for m in test_interest2_new]\n",
    "X_test['interest2_new'] = test_interest2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interest1_new = [' '.join(m) for m in train_interest1_new]\n",
    "X_train['interest1_new'] = train_interest2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interest1_new = [' '.join(m) for m in test_interest1_new]\n",
    "X_test['interest1_new'] = test_interest1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_new = pd.concat([X_train[['uid', 'interest2_new']], X_test[['uid', 'interest2_new']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_new1 = pd.concat([X_train[['uid', 'interest1_new']], X_test[['uid', 'interest1_new']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型调参  Baseline:0.703763  0.704483"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、加入原始所有特征后,  0.707527\n",
    "\n",
    "2、进行卡方检验降维后， 0.711182， 0.712489 0.712085  0.709924\n",
    "\n",
    "2、直接采用hyperot进行调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./datasets/train_data2.csv')\n",
    "target = pd.read_csv('./datasets/train_target2.csv', names=['label'], header=None)\n",
    "\n",
    "data.drop(data.columns[[0]], axis=1, inplace=True)  #删除某列函数\n",
    "target = target.reset_index(drop=True)\n",
    "\n",
    "one_hot_feature=['creativeSize', 'LBS','age','carrier','consumptionAbility', 'education','gender','house','os','marriageStatus','advertiserId','campaignId', 'creativeId',\n",
    "       'adCategoryId', 'productId', 'productType']\n",
    "vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']\n",
    "\n",
    "\n",
    "\n",
    "for feature in one_hot_feature:\n",
    "    try:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))\n",
    "    except:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split = StratifiedShuffleSplit(n_splits=1, test_size=0.12, random_state=42)\n",
    "y = np.array(target).squeeze()\n",
    "# for train_index, test_index in split.split(data,  y):\n",
    "#     X_train = data.iloc[train_index]\n",
    "#     X_test = data.iloc[test_index]\n",
    "#     y_train = y[train_index]\n",
    "#     y_test = y[test_index]\n",
    "X_train,X_test, y_train, y_test = train_test_split(data, y, test_size=0.3,random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clicked = X_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理联网类型特征\n",
    "ct_train = X_train['ct'].values\n",
    "ct_train = [m.split(' ') for m in ct_train]\n",
    "ct_trains = []\n",
    "for i in ct_train:\n",
    "    index = [0, 0, 0, 0, 0]\n",
    "    for j in i:\n",
    "        index[int(j)] = 1\n",
    "    ct_trains.append(index)\n",
    "\n",
    "ct_test = X_test['ct'].values\n",
    "ct_test = [m.split(' ') for m in ct_test]\n",
    "ct_tests = []\n",
    "for i in ct_test:\n",
    "    index = [0, 0, 0, 0, 0]\n",
    "    for j in i:\n",
    "        index[int(j)] = 1\n",
    "    ct_tests.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ad = X_train['aid'].value_counts().sort_index()\n",
    "num_ad_clicked = data_clicked['aid'].value_counts().sort_index()\n",
    "\n",
    "ratio = num_ad_clicked / num_ad\n",
    "\n",
    "ratio_clicked = pd.DataFrame({\n",
    "    'aid': ratio.index,\n",
    "    'ratio_clicked' : ratio.values\n",
    "})\n",
    "X_train = pd.merge(X_train, ratio_clicked, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, ratio_clicked, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_advertise_touser = X_train.groupby('aid').uid.nunique()\n",
    "num_advertise_touser = pd.DataFrame({\n",
    "    'aid': num_advertise_touser.index,\n",
    "    'num_advertise_touser' : num_advertise_touser.values\n",
    "})\n",
    "X_train = pd.merge(X_train, num_advertise_touser, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, num_advertise_touser, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_campaign = X_train['campaignId'].value_counts().sort_index()\n",
    "num_campaign_clicked = data_clicked['campaignId'].value_counts().sort_index()\n",
    "\n",
    "ratio_num_campaign = num_campaign_clicked / num_campaign\n",
    "\n",
    "ratio_num_campaign = pd.DataFrame({\n",
    "    'campaignId': ratio_num_campaign.index,\n",
    "    'ratio_num_campaign' : ratio_num_campaign.values\n",
    "})\n",
    "X_train = pd.merge(X_train, ratio_num_campaign, on=['campaignId'], how='left')\n",
    "X_test = pd.merge(X_test, ratio_num_campaign, on=['campaignId'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析每个广告对应的人群年龄分布, 教育水平分布，消费能力分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_toother(typename):\n",
    "    num_ad_totype = X_train.groupby('aid')[typename].value_counts()\n",
    "    num_ad_totype_clicked = data_clicked.groupby('aid')[typename].value_counts()\n",
    "    ratio_num_ad_totype = num_ad_totype_clicked / num_ad_totype\n",
    "    list_num_ad_totype = []\n",
    "    num_adid = X_train['aid'].value_counts().sort_index().index\n",
    "    for aid_out in num_adid:\n",
    "        dict_buf = {}\n",
    "        dict_num_ad_totype = {}\n",
    "        dict_num_ad_totype['aid'] = aid_out\n",
    "        for i, j in ratio_num_ad_totype.items():\n",
    "            aid = i[0]\n",
    "            feature = i[1]\n",
    "            if(aid == aid_out):\n",
    "                dict_buf[feature] = float(\"%.5f\" % j)\n",
    "        fea_name = 'num_ad_to'+typename\n",
    "        dict_num_ad_totype[fea_name] = dict_buf\n",
    "        list_num_ad_totype.append(dict_num_ad_totype)\n",
    "    return list_num_ad_totype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_ad_toedu = get_ad_toother('education')\n",
    "list_num_ad_toedu = pd.DataFrame(list_num_ad_toedu)\n",
    "X_train = pd.merge(X_train, list_num_ad_toedu, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_toedu, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_toedu'] = [ j.get(i, 0) for i, j in X_train[['education', 'num_ad_toeducation']].values]\n",
    "X_test['ratio_num_ad_toedu'] = [ j.get(i, 0) for i, j in X_test[['education', 'num_ad_toeducation']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_ad_toage = get_ad_toother('age')\n",
    "list_num_ad_toage = pd.DataFrame(list_num_ad_toage)\n",
    "X_train = pd.merge(X_train, list_num_ad_toage, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_toage, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_toage'] = [ j.get(i, 0) for i, j in X_train[['age', 'num_ad_toage']].values]\n",
    "X_test['ratio_num_ad_toage'] = [ j.get(i, 0) for i, j in X_test[['age', 'num_ad_toage']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_ad_toconsume = get_ad_toother('consumptionAbility')\n",
    "list_num_ad_toconsume = pd.DataFrame(list_num_ad_toconsume)\n",
    "X_train = pd.merge(X_train, list_num_ad_toconsume, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_toconsume, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_toconsume'] = [ j.get(i, 0) for i, j in X_train[['consumptionAbility', 'num_ad_toconsumptionAbility']].values]\n",
    "X_test['ratio_num_ad_toconsume'] = [ j.get(i, 0) for i, j in X_test[['consumptionAbility', 'num_ad_toconsumptionAbility']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_ad_tohouse = get_ad_toother('house')\n",
    "list_num_ad_tohouse = pd.DataFrame(list_num_ad_tohouse)\n",
    "X_train = pd.merge(X_train, list_num_ad_tohouse, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, list_num_ad_tohouse, on=['aid'], how='left')\n",
    "X_train['ratio_num_ad_tohouse'] = [ j.get(i, 0) for i, j in X_train[['house', 'num_ad_tohouse']].values]\n",
    "X_test['ratio_num_ad_tohouse'] = [ j.get(i, 0) for i, j in X_test[['house', 'num_ad_tohouse']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_interest(type_name, ratio):\n",
    "    num_adid = data_clicked['aid'].value_counts().sort_index().index\n",
    "    num_aid_clicked = dict(data_clicked['aid'].value_counts().sort_index())\n",
    "    num_user_clicksameAd_interest = data_clicked.groupby('aid')[type_name].value_counts()\n",
    "    dict_interest = {}\n",
    "    for adid in num_adid:\n",
    "        dict_buf = {}\n",
    "        for interest in num_user_clicksameAd_interest.items():\n",
    "            index = interest[0]\n",
    "            if index[0] == adid:\n",
    "                number = interest[1]\n",
    "                detail = index[1]\n",
    "                detail = detail.split(' ')\n",
    "                for det in detail:\n",
    "                    if det not in dict_buf:\n",
    "                        dict_buf[det] = number\n",
    "                    else:\n",
    "                        dict_buf[det] += number\n",
    "        dict_interest[adid] = dict_buf\n",
    "    dict_common_interest = []\n",
    "    for adid, dict_inter in dict_interest.items():\n",
    "        dict_common_buf = {}\n",
    "        dict_common_buf['aid'] = adid\n",
    "        common_inter = []\n",
    "        ad_total = num_aid_clicked[adid] - dict_inter.get('-1', 0)\n",
    "        if '-1' in dict_inter:\n",
    "            dict_inter.pop('-1')\n",
    "        for id_inter, num in dict_inter.items():\n",
    "            if num >= ad_total*ratio:\n",
    "                common_inter.append(id_inter)\n",
    "        str_name = 'common_'+type_name\n",
    "        dict_common_buf[str_name] = common_inter\n",
    "        dict_common_interest.append(dict_common_buf)\n",
    "    return dict_common_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_interest2 = get_common_interest('interest2', 0.25)\n",
    "df_common_interest2 = pd.DataFrame(dict_common_interest2)\n",
    "X_train = pd.merge(X_train, df_common_interest2, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_interest2, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_interest1 = get_common_interest('interest1', 0.25)\n",
    "df_common_interest1 = pd.DataFrame(dict_common_interest1)\n",
    "X_train = pd.merge(X_train, df_common_interest1, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_interest1, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_interest5 = get_common_interest('interest5', 0.25)\n",
    "df_common_interest5 = pd.DataFrame(dict_common_interest5)\n",
    "X_train = pd.merge(X_train, df_common_interest5, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_interest5, on=['aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_interest2'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_train[['interest2', 'common_interest2']].values]\n",
    "X_test['num_common_interest2'] = [len(set(i.split(' ')).intersection(set(j))) /len(j) for i, j in X_test[['interest2', 'common_interest2']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_interest1'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_train[['interest1', 'common_interest1']].values]\n",
    "X_test['num_common_interest1'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_test[['interest1', 'common_interest1']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_common_interest5'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_train[['interest5', 'common_interest5']].values]\n",
    "X_test['num_common_interest5'] = [len(set(i.split(' ')).intersection(set(j))) / len(j) for i, j in X_test[['interest5', 'common_interest5']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_common_topic1 = get_common_interest('topic1', 0.1)\n",
    "df_common_topic1 = pd.DataFrame(dict_common_topic1)\n",
    "X_train = pd.merge(X_train, df_common_topic1, on=['aid'], how='left')\n",
    "X_test = pd.merge(X_test, df_common_topic1, on=['aid'], how='left')\n",
    "X_train['num_common_topic1'] = [len(set(i.split(' ')).intersection(set(j))) / (len(j)+1) for i, j in X_train[['topic1', 'common_topic1']].values ]\n",
    "X_test['num_common_topic1'] = [len(set(i.split(' ')).intersection(set(j))) /  (len(j)+1) for i, j in X_test[['topic1', 'common_topic1']].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = ct_trains\n",
    "X_test_encoded = ct_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "oc_encoder = OneHotEncoder()\n",
    "for feature in one_hot_feature:\n",
    "    oc_encoder.fit(data[feature].reshape(-1, 1))\n",
    "    train_a=oc_encoder.transform(X_train[feature].values.reshape(-1, 1))\n",
    "    X_train_encoded = sparse.hstack((X_train_encoded, train_a))\n",
    "    test_a=oc_encoder.transform(X_test[feature].values.reshape(-1, 1))\n",
    "    X_test_encoded = sparse.hstack((X_test_encoded, test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']\n",
    "ct_encoder = CountVectorizer(min_df=0.0009)\n",
    "for feature in vector_feature:\n",
    "    ct_encoder.fit(data[feature])\n",
    "    train_a = ct_encoder.transform(X_train[feature])\n",
    "    X_train_encoded = sparse.hstack((X_train_encoded, train_a))\n",
    "    test_a = ct_encoder.transform(X_test[feature])\n",
    "    X_test_encoded = sparse.hstack((X_test_encoded, test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659912, 5114)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[['num_advertise_touser', 'ratio_clicked','num_common_interest2', 'num_common_interest1', 'num_common_interest5', 'num_common_topic1',\n",
    "                      'ratio_num_ad_toage' ,'ratio_num_ad_toconsume', 'ratio_num_ad_tohouse', 'ratio_num_campaign']].values)\n",
    "train_encoded = scaler.transform(X_train[['num_advertise_touser', 'ratio_clicked', 'num_common_interest2', 'num_common_interest1', 'num_common_interest5', \n",
    "                      'num_common_topic1', 'ratio_num_ad_toage', 'ratio_num_ad_toconsume', 'ratio_num_ad_tohouse', 'ratio_num_campaign']].values)\n",
    "\n",
    "test_encoded = scaler.transform(X_test[['num_advertise_touser', 'ratio_clicked','num_common_interest2', 'num_common_interest1', 'num_common_interest5', \n",
    "                        'num_common_topic1', 'ratio_num_ad_toage', 'ratio_num_ad_toconsume', 'ratio_num_ad_tohouse', 'ratio_num_campaign']].values)\n",
    "\n",
    "# scaler.fit(X_train[['num_advertise_touser']].values)\n",
    "# train_encoded = scaler.transform(X_train[['num_advertise_touser']].values)\n",
    "# test_encoded = scaler.transform(X_test[['num_advertise_touser']].values) \n",
    "# ,'num_common_interest2', 'num_common_interest1', 'num_common_interest5', 'num_common_topic1'  'ratio_num_ad_toconsume' \n",
    "X_train_encoded2 = sparse.hstack((X_train_encoded, train_encoded))\n",
    "X_test_encoded2 = sparse.hstack((X_test_encoded, test_encoded))\n",
    "\n",
    "# X_train_encoded2 = X_train_encoded\n",
    "# X_test_encoded2 = X_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "model = SelectKBest(chi2, k=4000).fit(X_train_encoded, y_train)\n",
    "X_train_new = model.transform(X_train_encoded)\n",
    "X_test_new = model.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练用卡方检验选取的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded3 = sparse.hstack((X_train_new, train_encoded))\n",
    "X_test_encoded3 = sparse.hstack((X_test_new, test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.63366\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\tvalid_0's auc: 0.636505\n",
      "[3]\tvalid_0's auc: 0.640027\n",
      "[4]\tvalid_0's auc: 0.649399\n",
      "[5]\tvalid_0's auc: 0.649131\n",
      "[6]\tvalid_0's auc: 0.651876\n",
      "[7]\tvalid_0's auc: 0.652166\n",
      "[8]\tvalid_0's auc: 0.653423\n",
      "[9]\tvalid_0's auc: 0.654501\n",
      "[10]\tvalid_0's auc: 0.655838\n",
      "[11]\tvalid_0's auc: 0.656319\n",
      "[12]\tvalid_0's auc: 0.658282\n",
      "[13]\tvalid_0's auc: 0.658428\n",
      "[14]\tvalid_0's auc: 0.658616\n",
      "[15]\tvalid_0's auc: 0.659008\n",
      "[16]\tvalid_0's auc: 0.659754\n",
      "[17]\tvalid_0's auc: 0.659946\n",
      "[18]\tvalid_0's auc: 0.660957\n",
      "[19]\tvalid_0's auc: 0.661452\n",
      "[20]\tvalid_0's auc: 0.662046\n",
      "[21]\tvalid_0's auc: 0.661842\n",
      "[22]\tvalid_0's auc: 0.661727\n",
      "[23]\tvalid_0's auc: 0.662151\n",
      "[24]\tvalid_0's auc: 0.663517\n",
      "[25]\tvalid_0's auc: 0.663133\n",
      "[26]\tvalid_0's auc: 0.66321\n",
      "[27]\tvalid_0's auc: 0.663319\n",
      "[28]\tvalid_0's auc: 0.663621\n",
      "[29]\tvalid_0's auc: 0.663832\n",
      "[30]\tvalid_0's auc: 0.664211\n",
      "[31]\tvalid_0's auc: 0.664484\n",
      "[32]\tvalid_0's auc: 0.664718\n",
      "[33]\tvalid_0's auc: 0.664845\n",
      "[34]\tvalid_0's auc: 0.664834\n",
      "[35]\tvalid_0's auc: 0.66503\n",
      "[36]\tvalid_0's auc: 0.665094\n",
      "[37]\tvalid_0's auc: 0.665763\n",
      "[38]\tvalid_0's auc: 0.665879\n",
      "[39]\tvalid_0's auc: 0.666139\n",
      "[40]\tvalid_0's auc: 0.666251\n",
      "[41]\tvalid_0's auc: 0.666461\n",
      "[42]\tvalid_0's auc: 0.666996\n",
      "[43]\tvalid_0's auc: 0.667045\n",
      "[44]\tvalid_0's auc: 0.667378\n",
      "[45]\tvalid_0's auc: 0.667791\n",
      "[46]\tvalid_0's auc: 0.668195\n",
      "[47]\tvalid_0's auc: 0.668465\n",
      "[48]\tvalid_0's auc: 0.668398\n",
      "[49]\tvalid_0's auc: 0.668796\n",
      "[50]\tvalid_0's auc: 0.669199\n",
      "[51]\tvalid_0's auc: 0.669578\n",
      "[52]\tvalid_0's auc: 0.669593\n",
      "[53]\tvalid_0's auc: 0.669743\n",
      "[54]\tvalid_0's auc: 0.670002\n",
      "[55]\tvalid_0's auc: 0.670256\n",
      "[56]\tvalid_0's auc: 0.670694\n",
      "[57]\tvalid_0's auc: 0.671546\n",
      "[58]\tvalid_0's auc: 0.671945\n",
      "[59]\tvalid_0's auc: 0.672418\n",
      "[60]\tvalid_0's auc: 0.672662\n",
      "[61]\tvalid_0's auc: 0.673144\n",
      "[62]\tvalid_0's auc: 0.673404\n",
      "[63]\tvalid_0's auc: 0.674128\n",
      "[64]\tvalid_0's auc: 0.674877\n",
      "[65]\tvalid_0's auc: 0.675228\n",
      "[66]\tvalid_0's auc: 0.675917\n",
      "[67]\tvalid_0's auc: 0.676678\n",
      "[68]\tvalid_0's auc: 0.677293\n",
      "[69]\tvalid_0's auc: 0.677724\n",
      "[70]\tvalid_0's auc: 0.678425\n",
      "[71]\tvalid_0's auc: 0.679109\n",
      "[72]\tvalid_0's auc: 0.679588\n",
      "[73]\tvalid_0's auc: 0.679964\n",
      "[74]\tvalid_0's auc: 0.680822\n",
      "[75]\tvalid_0's auc: 0.681235\n",
      "[76]\tvalid_0's auc: 0.681671\n",
      "[77]\tvalid_0's auc: 0.682187\n",
      "[78]\tvalid_0's auc: 0.682544\n",
      "[79]\tvalid_0's auc: 0.683169\n",
      "[80]\tvalid_0's auc: 0.683545\n",
      "[81]\tvalid_0's auc: 0.683911\n",
      "[82]\tvalid_0's auc: 0.684274\n",
      "[83]\tvalid_0's auc: 0.684617\n",
      "[84]\tvalid_0's auc: 0.685378\n",
      "[85]\tvalid_0's auc: 0.685994\n",
      "[86]\tvalid_0's auc: 0.686327\n",
      "[87]\tvalid_0's auc: 0.686703\n",
      "[88]\tvalid_0's auc: 0.687159\n",
      "[89]\tvalid_0's auc: 0.68756\n",
      "[90]\tvalid_0's auc: 0.688001\n",
      "[91]\tvalid_0's auc: 0.688355\n",
      "[92]\tvalid_0's auc: 0.688677\n",
      "[93]\tvalid_0's auc: 0.689114\n",
      "[94]\tvalid_0's auc: 0.689389\n",
      "[95]\tvalid_0's auc: 0.689795\n",
      "[96]\tvalid_0's auc: 0.690187\n",
      "[97]\tvalid_0's auc: 0.690618\n",
      "[98]\tvalid_0's auc: 0.690893\n",
      "[99]\tvalid_0's auc: 0.691122\n",
      "[100]\tvalid_0's auc: 0.691581\n",
      "[101]\tvalid_0's auc: 0.69188\n",
      "[102]\tvalid_0's auc: 0.692185\n",
      "[103]\tvalid_0's auc: 0.69264\n",
      "[104]\tvalid_0's auc: 0.693037\n",
      "[105]\tvalid_0's auc: 0.693352\n",
      "[106]\tvalid_0's auc: 0.693663\n",
      "[107]\tvalid_0's auc: 0.693975\n",
      "[108]\tvalid_0's auc: 0.694286\n",
      "[109]\tvalid_0's auc: 0.694696\n",
      "[110]\tvalid_0's auc: 0.695136\n",
      "[111]\tvalid_0's auc: 0.69549\n",
      "[112]\tvalid_0's auc: 0.695749\n",
      "[113]\tvalid_0's auc: 0.695929\n",
      "[114]\tvalid_0's auc: 0.696182\n",
      "[115]\tvalid_0's auc: 0.69633\n",
      "[116]\tvalid_0's auc: 0.6966\n",
      "[117]\tvalid_0's auc: 0.696801\n",
      "[118]\tvalid_0's auc: 0.697114\n",
      "[119]\tvalid_0's auc: 0.697372\n",
      "[120]\tvalid_0's auc: 0.697726\n",
      "[121]\tvalid_0's auc: 0.698099\n",
      "[122]\tvalid_0's auc: 0.698306\n",
      "[123]\tvalid_0's auc: 0.698693\n",
      "[124]\tvalid_0's auc: 0.698973\n",
      "[125]\tvalid_0's auc: 0.699187\n",
      "[126]\tvalid_0's auc: 0.699369\n",
      "[127]\tvalid_0's auc: 0.699606\n",
      "[128]\tvalid_0's auc: 0.699725\n",
      "[129]\tvalid_0's auc: 0.699837\n",
      "[130]\tvalid_0's auc: 0.700106\n",
      "[131]\tvalid_0's auc: 0.700414\n",
      "[132]\tvalid_0's auc: 0.700527\n",
      "[133]\tvalid_0's auc: 0.700731\n",
      "[134]\tvalid_0's auc: 0.700902\n",
      "[135]\tvalid_0's auc: 0.701051\n",
      "[136]\tvalid_0's auc: 0.701313\n",
      "[137]\tvalid_0's auc: 0.701462\n",
      "[138]\tvalid_0's auc: 0.701588\n",
      "[139]\tvalid_0's auc: 0.70195\n",
      "[140]\tvalid_0's auc: 0.70207\n",
      "[141]\tvalid_0's auc: 0.702272\n",
      "[142]\tvalid_0's auc: 0.702444\n",
      "[143]\tvalid_0's auc: 0.702585\n",
      "[144]\tvalid_0's auc: 0.7028\n",
      "[145]\tvalid_0's auc: 0.702884\n",
      "[146]\tvalid_0's auc: 0.703092\n",
      "[147]\tvalid_0's auc: 0.70324\n",
      "[148]\tvalid_0's auc: 0.703441\n",
      "[149]\tvalid_0's auc: 0.7036\n",
      "[150]\tvalid_0's auc: 0.703763\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's auc: 0.703763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "        learning_rate=0.05, max_depth=-1, min_child_samples=20,\n",
       "        min_child_weight=60, min_split_gain=0.0, n_estimators=150,\n",
       "        n_jobs=-1, num_leaves=31, objective='binary', random_state=2018,\n",
       "        reg_alpha=0.0, reg_lambda=0.9, silent=True, subsample=0.7,\n",
       "        subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train = np.array(target).squeeze()\n",
    "clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=0.9,\n",
    "        max_depth=-1, n_estimators=150, objective='binary',\n",
    "        subsample=0.7, colsample_bytree=0.8, subsample_freq=1,\n",
    "        learning_rate=0.05, min_child_weight=60, random_state=2018, n_jobs=-1\n",
    "    )\n",
    "clf.fit(X_train_encoded2, y_train, eval_set=[(X_test_encoded2, y_test)], eval_metric='auc',early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步参数调整，固定学习速率为0.05, 正则化参数默认, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import fmin, tpe, hp,space_eval,rand,Trials,partial,STATUS_OK\n",
    "\n",
    "def objective(argsDict):\n",
    "    num_leaves = argsDict[\"num_leaves\"] * 10 + 10\n",
    "    n_estimators = argsDict['n_estimators'] * 10 + 50\n",
    "#     learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    subsample = argsDict[\"subsample\"] * 0.1 + 0.7\n",
    "    colsample_bytree = argsDict[\"colsample_bytree\"] * 0.1 + 0.7\n",
    "    scale_pos_weight = argsDict[\"scale_pos_weight\"] + 1\n",
    "    min_child_weight = argsDict[\"min_child_weight\"] * 10 + 20\n",
    "    \n",
    "    print(\"num_leaves:\" + str(num_leaves))\n",
    "    print(\"n_estimator:\" + str(n_estimators))\n",
    "    print(\"subsample:\" + str(subsample))\n",
    "    print(\"colsample_bytree:\" + str(colsample_bytree))\n",
    "    print(\"scale_pos_weight:\" + str(scale_pos_weight))\n",
    "    print(\"min_child_weight:\" + str(min_child_weight))\n",
    "\n",
    "    gbm = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                             objective='binary',\n",
    "                             learning_rate=0.1, \n",
    "                             num_leaves=num_leaves, \n",
    "                             reg_alpha=0.0, \n",
    "                             reg_lambda=1,\n",
    "                             max_depth=-1,\n",
    "                             n_estimators=n_estimators, \n",
    "                             subsample=subsample, \n",
    "                             colsample_bytree=colsample_bytree, \n",
    "                             subsample_freq=1,\n",
    "                             min_child_weight=min_child_weight, \n",
    "                             scale_pos_weight=scale_pos_weight, \n",
    "                             random_state=2018, n_jobs=-1)\n",
    "    gbm.fit(X_train_encoded2, y_train, eval_set=[(X_test_encoded2, y_test)], eval_metric='auc',early_stopping_rounds=50)\n",
    "    metric = gbm.best_score_['valid_0']['auc']\n",
    "#     metric = cross_val_score(gbm, X_test_encoded2, y_train, cv=5,scoring=\"roc_auc\").mean()\n",
    "    print(metric)\n",
    "    return -metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves:50\n",
      "n_estimator:190\n",
      "subsample:0.7999999999999999\n",
      "colsample_bytree:0.7999999999999999\n",
      "scale_pos_weight:13\n",
      "min_child_weight:20\n",
      "[1]\tvalid_0's auc: 0.656776\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\tvalid_0's auc: 0.667452\n",
      "[3]\tvalid_0's auc: 0.671668\n",
      "[4]\tvalid_0's auc: 0.674301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-2319961142a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_startup_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-275-d71793ec61d7>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(argsDict)\u001b[0m\n\u001b[0;32m     32\u001b[0m                              \u001b[0mscale_pos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscale_pos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                              random_state=2018, n_jobs=-1)\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_encoded2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_encoded2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#     metric = cross_val_score(gbm, X_test_encoded2, y_train, cv=5,scoring=\"roc_auc\").mean()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    677\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    471\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1519\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1521\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "space = {\"num_leaves\":hp.randint(\"num_leaves\",10),\n",
    "         \"n_estimators\":hp.randint(\"n_estimators\", 50), \n",
    "         \"subsample\":hp.randint(\"subsample\",4), \n",
    "         \"colsample_bytree\":hp.randint(\"colsample_bytree\",4),\n",
    "         \"scale_pos_weight\": hp.randint(\"scale_pos_weight\", 20),\n",
    "         \"min_child_weight\": hp.randint(\"min_child_weight\",8),\n",
    "        }\n",
    "\n",
    "algo = partial(tpe.suggest,n_startup_jobs=1)\n",
    "best = fmin(objective,space,algo=algo,max_evals=500)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
